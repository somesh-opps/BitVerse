import base64
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import httpx

IMAGE_EXTENSIONS = {"jpg", "jpeg", "png", "webp", "bmp", "gif"}

app = FastAPI(
    title="Plant Disease Detection Local AI Server",
    description="API for receiving mobile app images and querying an external Ollama AI/ML server for diagnosis."
)

# Allow Mobile App to connect if it uses web views or browser testing
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration for Laptop 2 (AI/ML Server)
# Ollama runs locally on THIS laptop (Laptop 1 / hp)
OLLAMA_SERVER_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "llama3.2-vision:11b"
TEXT_MODEL_NAME = "llama3.2-vision:11b"  # same model handles text-only prompts

@app.post("/chat")
async def chat(request: dict):
    """
    Accepts a plain-text message from the mobile app,
    forwards it to the local Ollama model, and returns the AI reply.
    Expected body: { "message": "<user text>" }
    """
    message = request.get("message", "").strip()
    if not message:
        raise HTTPException(status_code=400, detail="message field is required.")

    system_context = (
        "You are CropIntel AI, an expert agricultural assistant specialising in "
        "crop diseases, nutrient deficiencies, pest management, soil health, and "
        "farming best practices. Give concise, practical, farmer-friendly advice."
    )

    payload = {
        "model": TEXT_MODEL_NAME,
        "prompt": f"{system_context}\n\nFarmer: {message}\nCropIntel AI:",
        "stream": False
    }

    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(OLLAMA_SERVER_URL, json=payload)
            response.raise_for_status()
            result = response.json()
            return {
                "success": True,
                "reply": result.get("response", "No response generated by model.")
            }
    except httpx.RequestError as exc:
        raise HTTPException(status_code=502, detail=f"Error communicating with Ollama. Is it running? Details: {exc}")
    except httpx.HTTPStatusError as exc:
        raise HTTPException(status_code=502, detail=f"Ollama returned an error: {exc.response.text}")
    except Exception as exc:
        raise HTTPException(status_code=500, detail=str(exc))

@app.post("/analyze-plant")
async def analyze_plant(image: UploadFile = File(...)):
    # Accept if content_type says image/* OR if the file extension is a known image type
    # (Flutter's http package sometimes sends application/octet-stream)
    ct = (image.content_type or "").lower()
    filename = (image.filename or "").lower()
    ext = filename.rsplit(".", 1)[-1] if "." in filename else ""
    is_image = ct.startswith("image/") or ext in IMAGE_EXTENSIONS
    if not is_image:
        raise HTTPException(status_code=400, detail=f"File is not a recognised image. content_type='{ct}', extension='{ext}'")

    # 1. Read image bytes
    image_bytes = await image.read()
    
    # 2. Encode image to Base64 (Ollama vision models expect Base64 encoded images)
    base64_image = base64.b64encode(image_bytes).decode("utf-8")

    # 3. Create a strict prompt instructing the model
    prompt = (
        "You are an expert agricultural botanist and plant pathologist. "
        "Analyze this image of a plant and identify any visible diseases, nutrient deficiencies, or pests. "
        "Provide a concise diagnosis, confidence level, and suggest a brief treatment or next step."
    )

    # 4. Prepare Payload for Ollama API
    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "images": [base64_image],
        "stream": False
    }

    try:
        # 5. Send POST request to Laptop 2 (Ollama Server)
        # We use a 120-second timeout since 11B vision models can take a moment to process inference.
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(OLLAMA_SERVER_URL, json=payload)
            response.raise_for_status()
            
            # 6. Parse the response from Ollama
            result = response.json()
            return {
                "success": True,
                "diagnosis": result.get("response", "No response generated by model.")
            }

    except httpx.RequestError as exc:
        raise HTTPException(status_code=502, detail=f"Error communicating with local AI server. Is Ollama running? Details: {exc}")
    except httpx.HTTPStatusError as exc:
        raise HTTPException(status_code=502, detail=f"AI server returned an error: {exc.response.text}")
    except Exception as exc:
        raise HTTPException(status_code=500, detail=str(exc))

if __name__ == "__main__":
    import uvicorn
    # Run the server on 0.0.0.0 so that the Mobile App can reach it over Wi-Fi
    uvicorn.run(app, host="0.0.0.0", port=8000)